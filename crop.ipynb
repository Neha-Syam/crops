{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neha-Syam/crops/blob/main/crop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVZ4ZhYWd2Id"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def load_and_preprocess_data(directory, img_size=(224, 224)):\n",
        "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        directory,\n",
        "        target_size=img_size,\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    validation_generator = datagen.flow_from_directory(\n",
        "        directory,\n",
        "        target_size=img_size,\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    return train_generator, validation_generator\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kaggle\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1slxyybGqpEf",
        "outputId": "067e4968-1a78-42d8-eaaf-66052927efbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmkkFsITqtvr",
        "outputId": "ad582af0-2a94-4ffb-8569-b880630826f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def load_json_data(data):\n",
        "    with open(data, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "data = load_json_data('/content/data.json')\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u05GAv7Fqtn4",
        "outputId": "7f7f72c0-fc81-44a9-cff5-5c66e5657402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'@context': {'@language': 'en', '@vocab': 'https://schema.org/', 'citeAs': 'cr:citeAs', 'column': 'cr:column', 'conformsTo': 'dct:conformsTo', 'cr': 'http://mlcommons.org/croissant/', 'data': {'@id': 'cr:data', '@type': '@json'}, 'dataBiases': 'cr:dataBiases', 'dataCollection': 'cr:dataCollection', 'dataType': {'@id': 'cr:dataType', '@type': '@vocab'}, 'dct': 'http://purl.org/dc/terms/', 'extract': 'cr:extract', 'field': 'cr:field', 'fileProperty': 'cr:fileProperty', 'fileObject': 'cr:fileObject', 'fileSet': 'cr:fileSet', 'format': 'cr:format', 'includes': 'cr:includes', 'isEnumeration': 'cr:isEnumeration', 'isLiveDataset': 'cr:isLiveDataset', 'jsonPath': 'cr:jsonPath', 'key': 'cr:key', 'md5': 'cr:md5', 'parentField': 'cr:parentField', 'path': 'cr:path', 'personalSensitiveInformation': 'cr:personalSensitiveInformation', 'recordSet': 'cr:recordSet', 'references': 'cr:references', 'regex': 'cr:regex', 'repeated': 'cr:repeated', 'replace': 'cr:replace', 'sc': 'https://schema.org/', 'separator': 'cr:separator', 'source': 'cr:source', 'subField': 'cr:subField', 'transform': 'cr:transform', 'wd': 'https://www.wikidata.org/wiki/'}, 'alternateName': '', 'conformsTo': 'http://mlcommons.org/croissant/1.0', 'license': {'@type': 'sc:CreativeWork', 'name': 'Unknown'}, 'distribution': [{'contentUrl': 'https://www.kaggle.com/api/v1/datasets/download/osamajalilhassan/agriculture-crops-dataset?datasetVersionNumber=1', 'contentSize': '78.993 MB', 'md5': 'pwuT4LlZNGTQjM3Xg0kUmA==', 'encodingFormat': 'application/zip', '@id': 'archive.zip', '@type': 'cr:FileObject', 'name': 'archive.zip', 'description': 'Archive containing all the contents of the Agriculture Crops Dataset dataset'}, {'includes': '*.(jpg|jpeg)', 'containedIn': {'@id': 'archive.zip'}, 'encodingFormat': 'image/jpeg', '@id': 'image-jpeg_fileset', '@type': 'cr:FileSet', 'name': 'image/jpeg files', 'description': 'image/jpeg files contained in archive.zip'}, {'includes': '*.png', 'containedIn': {'@id': 'archive.zip'}, 'encodingFormat': 'image/png', '@id': 'image-png_fileset', '@type': 'cr:FileSet', 'name': 'image/png files', 'description': 'image/png files contained in archive.zip'}], 'version': 1, 'keywords': [], 'isAccessibleForFree': True, 'includedInDataCatalog': {'@type': 'sc:DataCatalog', 'name': 'Kaggle', 'url': 'https://www.kaggle.com'}, 'creator': {'@type': 'sc:Person', 'name': 'Osama Jalil', 'url': '/osamajalilhassan', 'image': 'https://storage.googleapis.com/kaggle-avatars/thumbnails/8274135-kg.png'}, 'publisher': {'@type': 'sc:Organization', 'name': 'Kaggle', 'url': 'https://www.kaggle.com/organizations/kaggle', 'image': 'https://storage.googleapis.com/kaggle-organizations/4/thumbnail.png'}, 'thumbnailUrl': 'https://storage.googleapis.com/kaggle-datasets-images/3264177/5678019/c174e946da550da8a97c55d825748e33/dataset-card.jpg?t=2023-05-13-17-55-01', 'dateModified': '2023-05-13T17:51:14.4', 'datePublished': '2023-05-13T17:51:14.4', '@type': 'sc:Dataset', 'name': 'Agriculture Crops Dataset', 'url': 'https://www.kaggle.com/datasets/osamajalilhassan/agriculture-crops-dataset/versions/1', 'description': 'Agriculture crops refer to the various crops grown by farmers and other cultivators for food, fiber, and other purposes. These crops are an essential component of the global food supply chain, providing sustenance for humans and animals alike.\\nThe dataset contains separate folders for each type of crop, such as rice, sugarcane, maize, lemon, banana, coconut, jute, and many more. The images are in various sizes and resolutions, with different backgrounds and lighting conditions.\\nTo accomplish this task, you need to create a deep learning model that can classify each crop image with better accuracy. You can use a popular deep learning framework like TensorFlow, Keras, or PyTorch to build and train the model. You will also need to preprocess the images by resizing and normalizing the data to improve the accuracy of the model.\\nThe model should take input images from the dataset and predict the corresponding crop type. You can evaluate the accuracy of the model using standard metrics such as precision, recall, and F1-score. You can also visualize the results using confusion matrices and precision-recall curves.\\nTo get inspiration for this task, you can explore other resources related to image classification and crop identification. You can also use pre-trained models like VGG16, ResNet50, or InceptionV3 as a starting point for your model.\\nUltimately, the goal of this task is to create a model that can classify all types of agriculture crop images with high accuracy. This can have practical applications in various fields such as precision agriculture, crop monitoring, and disease detection.\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Assuming load_and_preprocess_data function is defined correctly\n",
        "def build_and_train_model(train_data, validation_data):\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False)\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    predictions = Dense(train_data.num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Freeze the layers of the base model\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(train_data, validation_data=validation_data, epochs=10)\n",
        "\n",
        "    # Save the model\n",
        "    model.save('crops.h5')  # HDF5 format\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Load and preprocess the data before model training\n",
        "# Ensure that 'data' is correctly replaced with your actual data path\n",
        "\n",
        "train_gen, val_gen = load_and_preprocess_data('/content/drive/MyDrive/Data')\n",
        "model = build_and_train_model(train_gen, val_gen)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSyROfsjd8ak",
        "outputId": "85d7d212-95d7-4514-ab18-498b911862f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 173 images belonging to 9 classes.\n",
            "Found 40 images belonging to 9 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "6/6 [==============================] - 14s 2s/step - loss: 2.2351 - accuracy: 0.3642 - val_loss: 1.9387 - val_accuracy: 0.4000\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 11s 2s/step - loss: 0.5753 - accuracy: 0.7919 - val_loss: 1.0383 - val_accuracy: 0.6500\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.1484 - accuracy: 0.9595 - val_loss: 1.3409 - val_accuracy: 0.5750\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 8s 1s/step - loss: 0.0644 - accuracy: 0.9884 - val_loss: 1.1023 - val_accuracy: 0.6250\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.9948 - val_accuracy: 0.7000\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.9757 - val_accuracy: 0.7250\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.9623 - val_accuracy: 0.7000\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.9646 - val_accuracy: 0.7000\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 8s 1s/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.9542 - val_accuracy: 0.7000\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 8s 1s/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.9286 - val_accuracy: 0.7250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load your existing TensorFlow model\n",
        "model = tf.keras.models.load_model('crops.h5')\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted model to a .tflite file\n",
        "with open('cropmodel.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n"
      ],
      "metadata": {
        "id": "ylm6WdPl2yoI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}